# -*- coding: utf-8 -*-
"""testcase_generation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIi960VTG_Upr1EVzAi-l71CvP04GFmk
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets accelerate sacrebleu

pip uninstall -y sentence-transformers transformers accelerate

pip install transformers==4.35.2 accelerate==0.25.0

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# =======================================================
# train_testcase.py â€” Testcase Generation Model (CodeT5+)
# =======================================================

import os, time
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sacrebleu import corpus_bleu
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    get_linear_schedule_with_warmup
)

# -------------------------------------------------------
# CONFIG
# -------------------------------------------------------
MODEL_NAME = "Salesforce/codet5p-220m"
CSV_PATH = "/content/drive/MyDrive/CodeFix_Datasets/testcase_generation.csv"
OUT_DIR = "/content/drive/MyDrive/models/testcase_generation"
os.makedirs(OUT_DIR, exist_ok=True)

MAX_SOURCE_LEN = 512
MAX_TARGET_LEN = 512
BATCH_SIZE = 4
EPOCHS = 60
LR = 2e-5
GRAD_ACCUM_STEPS = 4
WARMUP_RATIO = 0.1

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

# -------------------------------------------------------
# LOAD DATA
# -------------------------------------------------------
df = pd.read_csv(CSV_PATH)
df = df.dropna(subset=["input", "output"])
df = df[(df["input"].str.strip() != "") & (df["output"].str.strip() != "")]
df = df.reset_index(drop=True)

# 80/10/10 SPLIT
train_df, test_df = train_test_split(df, test_size=0.20, random_state=42)
train_df, val_df  = train_test_split(train_df, test_size=0.125, random_state=42)

print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

# -------------------------------------------------------
# TOKENIZER & MODEL
# -------------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
tokenizer.padding_side = "right"

model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model.gradient_checkpointing_enable()
model.to(DEVICE)

# -------------------------------------------------------
# DATASET
# -------------------------------------------------------
class TestcaseDataset(Dataset):
    def __init__(self, df):
        self.srcs = df["input"].astype(str).tolist()
        self.tgts = df["output"].astype(str).tolist()

    def __len__(self):
        return len(self.srcs)

    def __getitem__(self, idx):
        src = self.srcs[idx]
        tgt = self.tgts[idx]

        enc = tokenizer(
            src,
            max_length=MAX_SOURCE_LEN,
            truncation=True,
            padding=False
        )
        with tokenizer.as_target_tokenizer():
            dec = tokenizer(
                tgt,
                max_length=MAX_TARGET_LEN,
                truncation=True,
                padding=False
            )

        return {
            "input_ids": enc["input_ids"],
            "attention_mask": enc["attention_mask"],
            "labels": dec["input_ids"]
        }

from torch.nn.utils.rnn import pad_sequence
PAD = tokenizer.pad_token_id

def collate_fn(batch):
    ids = [torch.tensor(x["input_ids"]) for x in batch]
    att = [torch.tensor(x["attention_mask"]) for x in batch]
    lab = [torch.tensor(x["labels"]) for x in batch]

    ids = pad_sequence(ids, batch_first=True, padding_value=PAD)
    att = pad_sequence(att, batch_first=True, padding_value=0)
    lab = pad_sequence(lab, batch_first=True, padding_value=PAD)

    lab = lab.masked_fill(lab == PAD, -100)

    return {"input_ids": ids, "attention_mask": att, "labels": lab}

train_loader = DataLoader(TestcaseDataset(train_df), batch_size=BATCH_SIZE,
                          shuffle=True, collate_fn=collate_fn, num_workers=2)
val_loader = DataLoader(TestcaseDataset(val_df), batch_size=BATCH_SIZE,
                        shuffle=False, collate_fn=collate_fn, num_workers=2)

# -------------------------------------------------------
# OPTIMIZER + SCHEDULER
# -------------------------------------------------------
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
steps_per_epoch = (len(train_loader) + GRAD_ACCUM_STEPS - 1) // GRAD_ACCUM_STEPS
t_total = EPOCHS * steps_per_epoch
warmup = int(WARMUP_RATIO * t_total)

scheduler = get_linear_schedule_with_warmup(optimizer, warmup, t_total)
scaler = torch.cuda.amp.GradScaler()

# -------------------------------------------------------
# EVALUATION FUNCTION
# -------------------------------------------------------
def evaluate(model, loader, max_gen_tokens=256, max_batches=80):
    model.eval()
    preds, refs = [], []

    with torch.no_grad():
        for i, batch in enumerate(loader):
            if i >= max_batches:
                break

            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            out = model.generate(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_new_tokens=max_gen_tokens,
                num_beams=1
            )

            pred_text = tokenizer.batch_decode(out, skip_special_tokens=True)
            labels = batch["labels"].clone()
            labels[labels == -100] = PAD
            ref_text = tokenizer.batch_decode(labels, skip_special_tokens=True)

            preds.extend(pred_text)
            refs.extend(ref_text)

    bleu = corpus_bleu(preds, [refs]).score
    exact = sum(1 for p, r in zip(preds, refs) if p.strip() == r.strip())
    exact = exact / max(1, len(refs)) * 100
    return bleu, exact

# -------------------------------------------------------
# TRAINING LOOP + LOSS GRAPH
# -------------------------------------------------------
epoch_losses = []

for epoch in range(EPOCHS):
    print(f"\n===== Epoch {epoch+1}/{EPOCHS} =====")
    model.train()
    running_loss = 0
    optimizer.zero_grad()

    for step, batch in enumerate(train_loader):
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        with torch.cuda.amp.autocast():
            out = model(**batch)
            loss = out.loss / GRAD_ACCUM_STEPS

        scaler.scale(loss).backward()
        running_loss += loss.item() * GRAD_ACCUM_STEPS

        if (step + 1) % GRAD_ACCUM_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            scheduler.step()

    avg_loss = running_loss / len(train_loader)
    epoch_losses.append(avg_loss)
    print(f"Epoch Loss: {avg_loss:.4f}")

# -------------------------------------------------------
# SAVE MODEL
# -------------------------------------------------------
save_dir = os.path.join(OUT_DIR, "codet5_testcase_finetuned")
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("Saved model:", save_dir)

# -------------------------------------------------------
# LOSS GRAPH
# -------------------------------------------------------
plt.figure(figsize=(7,5))
plt.plot(range(1, EPOCHS+1), epoch_losses, marker='o', linewidth=2)
plt.title("Training Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

# testcase_predict.py
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ---------------------------------------------------
# Path to your fine-tuned testcase model
# ---------------------------------------------------
MODEL_PATH = "/content/drive/MyDrive/models/testcase_generation/codet5_testcase_finetuned"

# ---------------------------------------------------
# Load tokenizer + model
# ---------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

print("Loaded testcase generation model from:", MODEL_PATH)

# ---------------------------------------------------
# Prediction function
# ---------------------------------------------------
def generate_testcases(java_code, max_new_tokens=256):
    """
    Input  : Java function/class as string
    Output : Generated structured testcases (NOT code)
    """
    inputs = tokenizer(
        java_code,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            num_beams=5,            # more accurate testcase generation
            early_stopping=True
        )

    testcase_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return testcase_text.strip()

# ---------------------------------------------------
# Example Usage
# ---------------------------------------------------
java_code = """
public int add(int a, int b) {
    return a + b;
}
"""

print("\n===== INPUT JAVA CODE =====\n")
print(java_code)

print("\n===== GENERATED TESTCASES =====\n")
print(generate_testcases(java_code))